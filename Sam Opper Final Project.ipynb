{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom Classification - Safe to eat or deadly poison?\n",
    "\n",
    "Samuel Opper\n",
    "Dr. Dillon\n",
    "CSCI-4455\n",
    "12/17/18\n",
    "\n",
    "\n",
    "Mushrooms are low in calories and packed with nutrients like fiber and protein but there are over 70 deadly species and many closely resemble their edible counterparts. So,given an unlabeled tasty looking mushroomhow do you know if it’s safe to eat? Many people have solved this problem by training machine learning models to classify the mushrooms based on their physical featuresusing a dataset originally from the UCI machine learning repositoryand now on Kaggle. My plan was to compare logistic regression and random forest performances on this dataset since they are both very popular for binary classification. I have also compared my results to many Kaggleusers’  and a research paper published to IEEE.The mushroom dataset  consists of 8124 hypothetical samples drawn  from The Audubon Society Field Guide to North American Mushrooms (1981).323 species from two families are represented with 22 features like size shape color population and habitat each labeled as poison or edible.The data is overall reasonably balanced,and density plots show no perfect separation of features but good separation of spore print color, ring type, population, and habitat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are over 520 kernels on Kaggle attempting this dataset. Basically every technique has been applied by someone on Kaggle on this dataset.Most Kaggle users are successful and they have accuracies of 95-100%. Many have accuracies of 99-\n",
    "100%.The research paper I found compared decision tree, naïvebayes, and SVM on this dataset and concluded that the decision tree was the best. It had the highest accuracy of 100% along with SVM but it had faster processing speed.Their results can be seen in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I began my attempt by simply importing the dataset and training sklearns random forest model. To my suprise it had an accuracy of 100% on the testing set without any preprocessing or pruning. The decision tree model was just as good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to use SelectFromModel to create a subset of important features to train logistic regression and another decision tree. In order to avoid overfitting I split the dataset up multiple times. 20% of the dataset is used to test in the end, 24% was used for feature selection and the rest for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./mushrooms.csv')\n",
    "x = data.iloc[:,1:]\n",
    "y = data.iloc[:,0]\n",
    "d = defaultdict(LabelEncoder)\n",
    "xl = x.apply(lambda x: d[x.name].fit_transform(x))\n",
    "yl = LabelEncoder().fit_transform(y)\n",
    "\n",
    "x1, xTest, y1, yTest = train_test_split(xl,yl, test_size = 0.20)\n",
    "xTrain, xfeatureselect, yTrain, yfeatureselect = train_test_split(x1, y1, test_size = .30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree accuracy on prelim testing set:  1.0\n"
     ]
    }
   ],
   "source": [
    "xf, x2, yf, y2 = train_test_split(xfeatureselect,yfeatureselect, test_size = .1)\n",
    "dt = tree.DecisionTreeClassifier(criterion='entropy').fit(xf,yf)\n",
    "dtpred = dt.predict(x2)\n",
    "print('decision tree accuracy on prelim testing set: ', accuracy_score(y2,dtpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4549, 22)\n",
      "(4549, 5)\n",
      "selected features: \n",
      "odor gill-size gill-color spore-print-color population "
     ]
    }
   ],
   "source": [
    "select = SelectFromModel(dt, prefit = True)\n",
    "xTrain_s = select.transform(xTrain)\n",
    "xTest_s = select.transform(xTest)\n",
    "print(xTrain.shape)\n",
    "print(xTrain_s.shape)\n",
    "\n",
    "print('selected features: ')\n",
    "idx = select.get_support(indices = True)\n",
    "for i in idx:\n",
    "    print(x.columns[i], end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The decision tree usually selects from the following [odor gill-size gill-color stalk-root spore-print-color and population] . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression accuracy: 0.8646153846153846\n",
      "decision tree accuracy: 0.9956923076923077\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegressionCV(Cs = [.001,.01,.1,1,10,100], cv=10,random_state=5, penalty = 'l2', tol = .0001, scoring = 'precision', multi_class='ovr').fit(xTrain_s, yTrain)\n",
    "dt2 = tree.DecisionTreeClassifier(criterion='entropy').fit(xTrain_s, yTrain)\n",
    "\n",
    "lpred = logreg.predict(xTest_s)\n",
    "dpred = dt2.predict(xTest_s)\n",
    "\n",
    "print('logistic regression accuracy:',accuracy_score(yTest, lpred))\n",
    "print('decision tree accuracy:', accuracy_score(yTest, dpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### depending on randomness logistic regression model will score 85% +/-5% and decision tree will be 99% +/- 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    tn fp fn tp\n",
      "d  [833   0   7 785]\n",
      "log  [735  98 122 670]\n",
      "sensitivity,   specificity\n",
      "0.9911616161616161 1.0\n",
      "0.8459595959595959 0.8823529411764706\n"
     ]
    }
   ],
   "source": [
    "logcm = confusion_matrix(yTest, lpred).ravel()\n",
    "dcm = confusion_matrix(yTest,dpred).ravel()\n",
    "print('    tn fp fn tp')\n",
    "print('d ',dcm)\n",
    "print('log ',logcm)\n",
    "\n",
    "#tn fp fn tp\n",
    "dsense = dcm[3] / (dcm[3] + dcm[2])\n",
    "logsense = logcm[3] / (logcm[3] + logcm[2])\n",
    "\n",
    "dspec = dcm[0] / (dcm[0] + dcm[1])\n",
    "logspec = logcm[0] / (logcm[0] + logcm[1])\n",
    "\n",
    "print('sensitivity,   specificity')\n",
    "print(dsense , dspec)\n",
    "print(logsense, logspec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Overall decision tree is the best model for this dataset. It performs with 99.5% +/-.5% accuracy using only 4 out of the original 22 features but when the decision tree classifys incorrectly it is usually a false negative which is pretty bad for this purpose. If I had more time I would try to figure out how to improve the precision of the tree. The highest accuracy I could get logistic regression was about 90% and specifity of 91% which is not good enough for classifying something as edible or posion. The samples from this dataset were drawn up from a book about mushrooms and not actual recordings of real world samples so these models wont accurately predict if you go out and measure the physical features of an unlabeled mushroom you encounter. It does prove however that if you were to build a dataset of real world measurements and they follow the same patterns as in the mushroom book you could easily train a decision tree to accurately classify real world mushrooms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "\n",
    "1. Rabin18, R. C. (2018, January 19). What Is the Health and Nutritional Value of Mushrooms? Retrieved December 10, 2018, from https://www.nytimes.com/2018/01/19/well/eat/what-is-the-health-and-nutritional-value-of-mushrooms.htm\n",
    "\n",
    "2. Petruzello, M. (n.d.). 7 of the World’s Most Poisonous Mushroom. Retrieved December 10, 2018, from https://www.britannica.com/list/7-of-the-worlds-most-poisonous-mushrooms\n",
    "\n",
    "3. Mushrooms.csv[Csv]. (2016). Kaggle., from https://www.kaggle.com/uciml/mushroom-classification/homekaggle dataset\n",
    "\n",
    "4. Preda, G. (2017, November 22). Model Comparison for Mushrooms Classification. Retrieved December 10, 2018, from https://www.kaggle.com/gpreda/model-comparison-for-mushrooms-classification\n",
    "\n",
    "5. Aktas, B. (n.d.). Udemy Data Visualization Homework 1. Retrieved December 10, 2018, from https://www.kaggle.com/boheminsan/udemy-data-visualization-homework-1\n",
    "\n",
    "6. Wibowo, A., Y., A., & T. (2018, April 30). Classification Algorithm for Edible Mushroom Identification [Scholarly project]. In IEEE Xplore. Retrieved November 28, 2018, from https://ieeexplore.ieee.org/document/8350746"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
